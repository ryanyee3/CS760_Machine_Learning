\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb, bbm}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{multirow}
\makeatletter

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\title{CS 760 Midterm Note Sheet}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

\begin{center}{\huge{\textbf{CS 760 Midterm Note Sheet}}}\\
\end{center}
\begin{multicols*}{3}

\tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

% Overview
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $\mathcal{X} \rightarrow \text{input space}$ \\
        $\mathcal{Y} \rightarrow \text{output space}$ \\
        $\mathcal{H} \rightarrow \text{hypothesis class}$\\
        \textbf{Goal:} \\
        model $h \in \mathcal{H}$ that best approximates $f:\mathcal{X} \rightarrow \mathcal{Y}$
    \end{minipage}
};
% Overview Header
\node[fancytitle, right=10pt] at (box.north west) {Overview};
\end{tikzpicture}

% Empirical Risk Minimization
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             $\hat{f} = \arg \underset{h \in \mathcal{H}}{\min} \frac{1}{n} \sum_{i=1}^{n} \ell (h(x^{(i)}), y^{(i)})$
        \end{minipage}
    };
% Empirical Risk Minimization Header
\node[fancytitle, right=10pt] at (box.north west) {Empirical Risk Minimization};
\end{tikzpicture}

% k-Nearest Neighbors
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             $\hat{y} = \sum_{i=1}^{k} y^{(i)}$ for $k$ closest points \\
             \textbf{Distances:} \\
             Hamming: $d_{H}(x^{(i), x^{(j)}}) = \sum_{a=1}^{d} \mathbbm{1}(x^{(i)}_{a} \neq x^{(j)}_{a}) $ \\
             Euclidean: $d(x^{(i)}, x^{(j)}) = \left(\sum_{a=1}^{d} (x^{(i)}_{a} - x^{(j)}_{a})^{2} \right)^{-\frac{1}{2}}$ \\
             Manhattan: $d(x^{(i)}, x^{(j)}) = \sum_{a=1}^{d} |x^{(i)}_{a} - x^{(j)}_{a})^{2}|$
        \end{minipage}
    };
% k-Nearest Neighbors Header
\node[fancytitle, right=10pt] at (box.north west) {$k$ Nearest Neighbors};
\end{tikzpicture}

% Information Theory
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             Entropy: $H(Y) = -\underset{y \in Y}{\sum} \mathbb{P}(y) \log_{2}(\mathbb{P}(y))$ \\
             Conditional: $H(Y \vert X) = \underset{x \in X}{\sum} \mathbb{P}(X=x) H(Y \vert X=x)$ \\
             Information Gain: $H(Y) - H(Y \vert X)$ \\
             Gain Ratio: $\frac{H(Y)-H(Y \vert S)}{H(S)}$
        \end{minipage}
    };
% Information Theory Header
\node[fancytitle, right=10pt] at (box.north west) {Information Theory};
\end{tikzpicture}

% Evaluation
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
            \begin{tabular}{l|l|c|c|c}
                \multicolumn{2}{c}{}&\multicolumn{2}{c}{Observed Class}&\\
                \cline{3-4}
                \multicolumn{2}{c|}{}&True&False&\multicolumn{1}{c}{}\\
                \cline{2-4}
                \multirow{2}{*}{Predicted Class}& True & $TP$ & $FP$ & \\
                \cline{2-4}
                & False & $FN$ & $TN$ & \\
                \cline{2-4}
            \end{tabular} \\
             Accuracy: $\frac{TP + TN}{TP + FP + TN + FN}$ \\
             Error: 1 - Accuracy \\
             Precision: $\frac{TP}{TP + FP}$ \\
             Recall: $\frac{TP}{TP + FN}$ \\
             False Positive Rate: $\frac{FP}{TN + FP}$ \\
             ROC Curve: recall (TPR) vs. false positive rate \\
             Precision/Recall Curve: downward sloping with asymptote
        \end{minipage}
    };
% Evaluation Header
\node[fancytitle, right=10pt] at (box.north west) {Evaluation};
\end{tikzpicture}

\vfil\null
\columnbreak

% Decision Trees
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             \textbf{Algorithm:} \\
             \textit{MakeSubtree}: \\
             Determine Candidate Splits \\
             if stopping criteria: make leaf \\
             else: \textit{MakeSubtree} with best split \\
             return subtree rooted at N
        \end{minipage}
    };
% Decision Trees Header
\node[fancytitle, right=10pt] at (box.north west) {Decision Trees};
\end{tikzpicture}

% Linear Regression
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             Loss: $\ell(f_{\theta}) = \frac{1}{n} ||\mathbf{X}\theta - y||^{2}_{2}$ \\ 
             Gradient: $\nabla_{\theta} = \frac{1}{n}(2\mathbf{X}^{T}\mathbf{X}\theta - 2\mathbf{X}^{T}y)$ \\
             Solution: $\theta = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}y$ \\
             Ridge Loss: $\ell(f_{\theta}) = \frac{1}{n} ||\mathbf{X}\theta - y||^{2}_{2} + \lambda||\theta||^{2}_{2}$ \\  
             Ridge Sol: $\theta = (\mathbf{X}^{T}\mathbf{X} + \lambda n I)^{-1}\mathbf{X}^{T}y$
        \end{minipage}
    };
% Linear Regression Header
\node[fancytitle, right=10pt] at (box.north west) {Linear Regression};
\end{tikzpicture}

% Logistic Regression
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
            Sigmoid: $\sigma(z) = \frac{1}{1+\exp(-z)} = \frac{\exp(z)}{1+\exp(z)} \in (0, 1)$ \\
            Properties: $1-\sigma(z) = \sigma(-z)$, $\sigma^{'}(z) = \sigma(z)(1-\sigma(z))$ \\
            Cross Entropy Loss: 
        \end{minipage}
        };
% Logistic Regression Header
\node[fancytitle, right=10pt] at (box.north west) {Logistic Regression};
\end{tikzpicture}

% Estimation 
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             \textbf{Maximum Likelihood:} \\
             $\hat{\theta}_{MLE} = \arg \underset{\theta}{\max} \mathcal{L}(\theta; Y, X)$ \\
             $\mathcal{L}(\theta; Y, X) = \prod_{i=1}^{n} \mathbb{P}_{\theta}(y_i \vert x_i)$ \\
             \textbf{Maximum a posteriori Probability:} \\
             $\hat{\theta}_{MAP} = \arg \underset{\theta}{\max} \prod_{i=1}^{n} p(x^{(i)} \vert \theta) p(\theta)$ 
        \end{minipage}
    };
% Estimation Header
\node[fancytitle, right=10pt] at (box.north west) {Estimation};
\end{tikzpicture}

% Gradient Descent
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.3\textwidth}
             \textbf{Convergence Criteria:} \\
             1. Convex: $f(\lambda x_{1} + (1-\lambda)x_{2}) \leq \lambda f(x_{1}) + (1-\lambda) f(x_{2})$ \\
             2. Differentiable \\
             3. Lipschitz-continuous: $\nabla^{2}f(x) \preccurlyeq LI$ \\
             recall: ($B-A$ is positive semidefinite if $A \preccurlyeq B$) \\
             recall: if $C$ is positive semidefinite then $x^{T}Cx \geq 0$ $\forall x$
        \end{minipage}
    };
% Gradient Descent
\node[fancytitle, right=10pt] at (box.north west) {Gradient Descent};
\end{tikzpicture}

\end{multicols*}
\end{document}

% % New Section 
% \begin{tikzpicture}
%     \node [mybox] (box){%
%         \begin{minipage}{0.3\textwidth}
%              .
%         \end{minipage}
%     };
% % New Header
% \node[fancytitle, right=10pt] at (box.north west) {New Header};
% \end{tikzpicture}
